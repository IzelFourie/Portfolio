[
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#goal",
    "href": "posts/classification_adult_data/index_adult_data.html#goal",
    "title": "Statistical Learning: classification problem",
    "section": "Goal",
    "text": "Goal\nIn this post I use different statistical models for predicting whether a person’s annual income will exceed $50K a year based on data in the “Census Income” data set.\n\n\n\n\n\nCode\n# Load the necessary libraries\n# car: VIF factor\n# caret: confusionMatrix\n# naniar: missing values\n# e1071: support vector classifier\n# ROCR: ROC curves\n\npacman::p_load(\"qgg\", \"corrplot\", \"ggplot2\", \"tidyr\", \"mlbench\", \n               \"readr\", \"data.table\", \"naniar\", \"car\", \"caret\",\n                \"xgboost\", \"dplyr\", \"Matrix\", \"e1071\", \"glmnet\",\n               \"ROCR\", \"pROC\")\n\n\n\nThe focus of this post is to determine which statistical model gives the best prediction of this binary outcome. I will compare logistic regression, XGBoost and the Support Vector Classifier models for their accuracy and precision in predicting the outcome. I will not spend a lot of time on explorative data analysis."
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#data",
    "href": "posts/classification_adult_data/index_adult_data.html#data",
    "title": "Statistical Learning: classification problem",
    "section": "Data",
    "text": "Data\nThe data as well as information about the dataset can be found here.\n\n\nCode\n# Download the Adult data\nurl <- \"https://archive.ics.uci.edu/static/public/2/adult.zip\"\ndestfile <- tempfile()\nexdir <- tempdir()\ndownload.file(url=url, destfile=destfile)\nunzip(destfile, exdir=exdir)\n# I am not showing the output of the following. It is not pretty to look at, but helps me identify the files downloaded in the zipped file.\nlist.files(exdir, full.names = TRUE) \n\n\nThe Adult data comes in a train and test set, so no need to partition the data into a training and test set.\n\n\nCode\n# Read the train and test data sets\ndf_train <- fread(file.path(exdir,\"adult.data\"), data.table=FALSE)\ndf_test <- fread(file.path(exdir,\"adult.test\"), skip=1, data.table=FALSE)\n\n\nThe variable names are given in the description of the data on the website. I added them manually.\n\n\nCode\ncolnames(df_train) <- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n                         \"marital_status\", \"occupation\", \"relationship\", \"race\",\n                         \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\",\n                         \"native_country\", \"income\")\n\ncolnames(df_test) <- c(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n                         \"marital_status\", \"occupation\", \"relationship\", \"race\",\n                         \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\",\n                         \"native_country\", \"income\")\n\n# Look at the structure of the data frames\nstr(df_train)\nstr(df_test)\n\ndim(df_train)\n\n# Clean the test dataset income column by remove the \".\"\n# Use fixed=TRUE to treat it as a literal string, and not a regular expression:\ndf_test$income <- gsub(\".\", \"\", df_test$income, fixed=TRUE)\n\n\nThe data consists of variables. The training data consists of 32561 observations and the test data of 16281.\nHere is a quick view of what the data looks like.\n\n\nCode\nhead(df_train)\n\n\n  age        workclass fnlwgt education education_num     marital_status\n1  39        State-gov  77516 Bachelors            13      Never-married\n2  50 Self-emp-not-inc  83311 Bachelors            13 Married-civ-spouse\n3  38          Private 215646   HS-grad             9           Divorced\n4  53          Private 234721      11th             7 Married-civ-spouse\n5  28          Private 338409 Bachelors            13 Married-civ-spouse\n6  37          Private 284582   Masters            14 Married-civ-spouse\n         occupation  relationship  race    sex capital_gain capital_loss\n1      Adm-clerical Not-in-family White   Male         2174            0\n2   Exec-managerial       Husband White   Male            0            0\n3 Handlers-cleaners Not-in-family White   Male            0            0\n4 Handlers-cleaners       Husband Black   Male            0            0\n5    Prof-specialty          Wife Black Female            0            0\n6   Exec-managerial          Wife White Female            0            0\n  hours_per_week native_country income\n1             40  United-States  <=50K\n2             13  United-States  <=50K\n3             40  United-States  <=50K\n4             40  United-States  <=50K\n5             40           Cuba  <=50K\n6             40  United-States  <=50K\n\n\nAlthough I didn’t want to spend too much time on explorative data analysis, I couldn’t help looking at the income difference between men and women. (Just for fun I created the graph in Tableau.) The figure shows exactly what I expected: that the proportion of females earning more than $50K would be much lower than the proportion of males. The figure also shows that there is class imbalance in the dataset, since the class income > $50K occurs less frequent than the other.\n\n                   \n\n\nThe figure below shows the distribution of ages for people earning more or less than $50,000 per year. There is a clear shift towards older age for earning above $50K. The curve suggests that in 1994 middle-aged individuals were more likely to have incomes above $50K.\n\n\nCode\n# dev.off()\n# To see the age range where people are more likely to fall into a particular income bracket:\nggplot(df_train, aes(x = age, fill = income)) +\n  geom_density(alpha = 0.7) +\n  labs(\n    title = \"Age distribution by income group\",\n    X = \"Age\", Y = \"Density\"\n  ) +\n  scale_fill_manual(values = c(\"<=50K\" = \"#4E7AA7\", \">50K\" = \"#E49343\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank()\n    #,\n    #panel.grid.minor = element_blank()\n    ) +\n  labs(title = \"Age Distribution by Income Group\",\n       x = \"Age\", y = \"Density\")"
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#data-preparation",
    "href": "posts/classification_adult_data/index_adult_data.html#data-preparation",
    "title": "Statistical Learning: classification problem",
    "section": "Data preparation",
    "text": "Data preparation\nJoin the training and test datasets so that I can edit the datasets as a whole.\n\n\nCode\n# Change the character variables to factors\ndf  <- rbind(df_train,df_test)\ncharacter_vars <- lapply(df, class) == \"character\"\nstr(df)\n\n\nEven though the website mentions missing data, there does not seem to be missing data in the train and test sets.\n\n\nCode\n# colSums(is.na(df_train))\n# colSums(is.na(df_test))\ncolSums(is.na(df))\n\n\nThere are duplicates in the data, but it is not unlikely that more than one person could have the same entries for the different variables in this dataset.\n\n\nCode\nany(duplicated(df))\nsum(duplicated(df) | duplicated(df, fromLast = TRUE))\n\n\nLook at the distribution of the numerical columns by making box plots.\n\n\nCode\n# Identify numeric colummns\n# Create a boxplot of each numeric column\nnumeric_columns <- df[, sapply(df, is.numeric)]\npar(mfrow = c(2, 3))  \nlapply(names(numeric_columns), function(col) {\n  boxplot(numeric_columns[[col]], main = col, ylab = col)\n})\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\nA closer look at the distribution of the capital_gain and capital_loss variables shows that their distribution is highly skewed to the right, with the majority of observations being zero. There are a few extreme outliers that are inflating the mean.\n\n\nCode\n# Summary of capital_gain and capital_loss\ncat(\"Summary of Capital Gain:\\n\")\n\n\nSummary of Capital Gain:\n\n\nCode\nsummary(df$capital_gain)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0    1079       0   99999 \n\n\nCode\ncat(\"\\nSummary of Capital Loss:\\n\")\n\n\n\nSummary of Capital Loss:\n\n\nCode\nsummary(df$capital_loss)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0     0.0     0.0    87.5     0.0  4356.0 \n\n\nI decide to change these variables to binary variables instead, where any value above zero is assigned “1” otherwise “0”.\n\n\nCode\ndf$capital_gain <- ifelse(df$capital_gain > 0, 1, 0)\ndf$capital_gain <- as.factor(df$capital_gain)\n\ndf$capital_loss <- ifelse(df$capital_loss > 0, 1, 0)\ndf$capital_loss <- as.factor(df$capital_loss)\n\n\nI check for covariance between variables (correlated features) and variables with very low variance (below 1e-5).\nThe highest correlation is observed between hours_per_week and education_num with a value of 0.148.\n\n\nCode\n## Look at covariance between variables\n# Select numerical features in the training data\nnum_vars <- sapply(df_train, is.numeric)\n\n# Compute correlation matrix for numerical variables\n# None of the variables appear to be highly correlated\ncor_matrix <- cor(df_train[, num_vars], use = \"complete.obs\")\ncor_matrix\n\n# Visualize correlations with a heatmap\ncorrplot(cor_matrix, method = \"color\", tl.cex = 0.7)\n\n\n\n\n\nHeatmap of correlations between numerical variables.\n\n\n\n\nCode\n## Look at the variance of numerical variables\n# Calculate variance for each numeric column\nvariances <- apply(df_train[, num_vars], 2, var)\n\n# Find features with low variance (e.g., variance close to 0)\nlow_variance_features <- names(variances[variances < 1e-5])\n\n## Check for variables consisting of a single value\nsingle_value_check <- sapply(df_train, function(x) length(unique(x)) == 1)\n\n\nThe number of observations for the different outcomes for predictor variable classes is important. It is preferable to have at least 10 observations for each level of the outcome variable for each of the predictor variable categories.\n\n\nCode\n## Look at the number of observations for different classes of categorical variables\nsapply(df_train[,character_vars], table)\n\n# Create contingency tables between each character variable and df$income\ncharacter_vars_pred <- setdiff(names(df_train)[sapply(df_train, is.character)], \"income\")\ncontingency_tables <- lapply(character_vars_pred, function(var) {\n  table(df_train[[var]], df_train$income)\n})\n\n# Name each table by the character variable\nnames(contingency_tables) <- character_vars_pred\n\n# View the contingency tables\ncontingency_tables\n\n\nI decide to combine some of the categories for the variables workclass, education, occupation and native_country. These variables are edited as follows:\nworkclass\nWorking Without_pay and Never_worked is very unlikely to result in an income above >50K. I am merging these categories with the category “?”.\n\n\nCode\ndf$workclass[df$workclass %in% c(\"Without-pay\", \"Never-worked\")] <- \"?\" \ntable(df$income, df$workclass)\n\n\neducation\nAdd Preschool, and the other grades up to 9th grade and call it pre-primary-school.\n\n\nCode\ndf$education[df$education %in% c(\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\", \"9th\")] <- \"pre-primary-school\"\ntable(df$income, df$education)\n\n\noccupation\nMove Armed-Forces and Priv-house-serv to “?”\n\n\nCode\ndf$occupation[df$occupation %in% c(\"Armed-Forces\", \"Priv-house-serv\")] <- \"?\"\ntable(df$income, df$occupation)\n\n\nnative_country\nMove all countries with that have less than 10 observations in the training data to a new category called “Other”.\n\n\nCode\n# Create a contingency table of income and native_country\ncontingency_table_native_country <- table(df_train$income, df_train$native_country)\n\n# Find the native_country categories with fewer than 10 observations in any income row\nlow_count_country <- apply(contingency_table_native_country, 2, function(x) any(x < 10))\n\n# Extract the countries that meet the condition\nlow_count_countries <- names(low_count_country[low_count_country])\n\n# Move all low count countries to a new category \"other\"\ndf$native_country[df$native_country %in% low_count_countries] <- \"Other\"\n\n\nOther minor adjustments: Create a binary variable for income,\n\n\nCode\ndf$income <- ifelse(df$income == \">50K\", 1, 0)\ndf$income <- as.factor(df$income)\n\n\nand change all character variables to factors.\n\n\nCode\ndf[, character_vars] <- lapply(df[, character_vars], as.factor)\n\n# Save the full dataframe at this point.\n# saveRDS(df, file = \"posts/classification_adult_data/data/df_adult_edited.rds\")\n\n\nCreate the train and test datasets.\n\n\nCode\n# Define train and test data indices\ntrain <- 1:nrow(df_train) \n# Add the number of rows in the test data to each row number of the training data in order to generate and index for the rows of the test data frame.\ntest <- (1:nrow(df_test))+ nrow(df_train) \n\n# Create train and test datasets\ndf_train <- df[train, ]\ndf_test <- df[test, ]"
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#analysis",
    "href": "posts/classification_adult_data/index_adult_data.html#analysis",
    "title": "Statistical Learning: classification problem",
    "section": "Analysis",
    "text": "Analysis\nI decided not to address the class-imbalance in these initial analyses. Due to the class imbalance I expect that the models may have low sensitivity, or in other words, struggle to correctly identify the minority class (in this case where income > 50K). I will therefore explicitly compare the sensitivity between the different models.\nSince the data contain a traning and test dataset, I decided to use the validation set approach for cross-validation. Thus, for each of the analyses, I fit the model on the training data and assess the model’s performance on the test data.\n\nLogistic Regression\nI first fit a logistic regression model, including all the variables in the dataset. There are some variables that have a significant influence on whether a person’s annual income is above $50K per year.\nThe variable education_num has a high variable inflation factor (VIF). It is likely highly correlated with the factor variable education. This variable is removed in subsequent analyses.\nAfter removing the education_num variable, education becomes significant. The workclass variable does not seem to have a significant influence on the outcome. A Chi2 test also reveals a significant association between the variables workclass and occupation. I remove this variable to see if it would improve the prediction model.\n\n\nCode\n### Models\n## Model 1 (formula 1 = f1): include all variables\n# R by default assigns the higher factor level as the positive class\nglmf1 <- income ~ .\nfit_glmf1 <- glm(glmf1, data = df_train, family = \"binomial\")\n# Look at the summary\nsummary(fit_glmf1)\n\n# Use the function alias to see if there may be collinearities in the data. It looks like there are none.\nalias(fit_glmf1)\n\n# The variance inflation factor of education_num is very high. I will remove education_num for subsequent analyses\nvif(fit_glmf1)\n\n## Model 2\n# Based on AUC and precision, this model is the best of the logistic regression models\nglmf2 <- income ~ . -education_num \nfit_glmf2 <- glm(glmf2, data = df_train, family = \"binomial\")\nsummary_glmf2 <- summary(fit_glmf2)\n\n# Looking at the variance inflation factor, it looks like there is no more problematic multicollinearity, since none of the adjusted VIF factors are above 5. Although there could be moderate multicollinearity between the variables relationship, marital_status and sex.\nvif(fit_glmf2)\n\n# Take a look at independence between workclass and occupation variables.\n# It could be worth dropping one of these variables, as the Chi-square test shows significant association between them.\nchisq.test(table(df_train$workclass, df_train$occupation))\n\n## Model 3\nglmf3 <- income ~ . -education_num -workclass \nfit_glmf3 <- glm(glmf3, data = df_train, family = \"binomial\")\nsummary(fit_glmf3)\nvif(fit_glmf3)\n\n\nMake predictions using logistic regression Model 2 and Model 3 and determine model performance in the test data.\n\n\nCode\n# Predict the probabilities using the logistic regression models\nprobs_glmf2 <- predict(fit_glmf2, newdata = df_test, type = \"response\")\nprobs_glmf3 <- predict(fit_glmf3, newdata = df_test, type = \"response\")\n\n## Predict the class based on the calculated probabilities\n# Set predicted class to \">50K\" for observations with probabilities greater than 0.5\npred_glmf2 <- rep(\"0\", 16281)\npred_glmf2[probs_glmf2 > 0.5] <- \"1\" \ntable(pred_glmf2, df_test$income)\n\npred_glmf3 <- rep(\"0\", 16281)\npred_glmf3[probs_glmf3 > 0.5] <- \"1\" \ntable(pred_glmf3, df_test$income)\n\n## Create confusion matrices to determine accuracy, sensitivity etc.\nconfmatrix_glmf2 <- confusionMatrix(table(pred_glmf2, df_test$income), positive = \"1\")\nconfmatrix_glmf3 <- confusionMatrix(table(pred_glmf3, df_test$income), positive = \"1\")\n\n# Extract different metrics for model performance\n# Specificity = TN/(TN + FP)\n# Accuracy = TP + TN/(TP + TN + FP + FN)\n# Precision = TP/(TP + FP)\n# Precision is given as \"Pos Pred Value\" in the caret package\nspecificity_glmf2 <- confmatrix_glmf2$byClass[\"Specificity\"]\nsensitivity_glmf2 <- confmatrix_glmf2$byClass[\"Sensitivity\"]\naccuracy_glmf2 <- as.numeric(confmatrix_glmf2$overall[\"Accuracy\"])\nprecision_glmf2 <- round(as.numeric(confmatrix_glmf2$byClass[\"Pos Pred Value\"]), 3)\n\nspecificity_glmf3 <- confmatrix_glmf3$byClass[\"Specificity\"]\naccuracy_glmf3 <- as.numeric(confmatrix_glmf3$overall[\"Accuracy\"])\nprecision_glmf3 <- round(as.numeric(confmatrix_glmf3$byClass[\"Pos Pred Value\"]), 3)\n\n# Calculate the area under the curve\n# The predicted outcome must be numeric\ny <- as.numeric(df_test$income)-1\n\nroc_glmf2 <- roc(y, probs_glmf2)\nauc_glmf2 <- roc_glmf2$auc\n\nroc_glmf3 <- roc(y, probs_glmf3)\nauc_glmf3 <- roc_glmf3$auc\n\n# Quick comparison between the AUC and precision of the two models.\n# Model 2 show the best model performance.\nprint(c(auc_glmf2, auc_glmf3))\n\nprint(c(confmatrix_glmf2$byClass[\"Pos Pred Value\"], confmatrix_glmf3$byClass[\"Pos Pred Value\"]))\n\n# Use the prediction function from the ROCR package to create a prediction object. This will be used later to create an ROC graph\n# Create a performance object for the ROC curve\n# tpr = true positive rate, fpr = false positive rate\npred_object_glmf2 <- prediction(probs_glmf2, df_test$income)\nperf_glmf2 <- performance(pred_object_glmf2, \"tpr\", \"fpr\")\nauc_glmf2 <- performance(pred_object_glmf2, measure = \"auc\")@y.values[[1]]\n\n\nTake a look at which coefficients are most important for making the prediction.\n\n\nCode\n# Look at the coefficients of model 2\n# Extract coefficients (includes estimates, standard errors, etc.) for model 2\n# To order the coefficients by their absolute estimate size:\ncoefficients_glmf2 <- summary_glmf2$coefficients\nordered_by_estimate <- coefficients_glmf2[order(abs(coefficients_glmf2[, \"Estimate\"]), decreasing = TRUE), ]\n\n# Create a dataframe of variable names and estimates\n# Use the original estimates, not absolute values\nimportance_glmf2 <- data.frame(\n  Variable = rownames(ordered_by_estimate),  \n  Estimate = ordered_by_estimate[, \"Estimate\"]  \n)\n\n# Remove the intercept from the dataframe\n# Subset to the top 20 estimates (after removing intercept)\nimportance_glmf2_no_intercept <- importance_glmf2[rownames(importance_glmf2) != \"(Intercept)\", ]\nimportance_glmf2_top20 <- importance_glmf2_no_intercept[1:20, ]\n\n# Plot top 20 absolute largest estimates\n# Flip coordinates to make the plot horizontal\nimportance_plot_glm <- ggplot(importance_glmf2_top20, aes(x = reorder(Variable, abs(Estimate)), y = Estimate)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(Estimate > 0, \"Positive\", \"Negative\"))) +\n  coord_flip() +  \n  labs(title = \"Logistic Regression Variable Importance\",\n       x = \"Variables\",\n       y = \"Coefficient Estimates\") +\n  theme_minimal() +\n  # Define custom colors for positive (blue) and negative (orange)\n  scale_fill_manual(values = c(\"Negative\" = \"darkorange\", \"Positive\" = \"steelblue\"), guide = \"none\") +\ntheme(axis.text.x = element_text(size = 12),  \n      axis.text.y = element_text(size = 12))\n\n\n\n\nRidge Regression\nI suspect that there may still be some correlated features in the data, such as relationship and marital_status. I use Ridge regression to distribute the weight more evenly across the features. The goal of this exercise is to determine which model will give the best prediction accuracy. Thus, model simplicity is not the biggest concern.\nTo do Ridge regression the data needs to be converted to a model matrix.\n\n\nCode\n# Create a model matrix using the model.matrix() function, which converts factors into dummy variables and represent all predictor variables in a matrix format.\n# model.matrix() is used to one-hot encode categorical variables - i.e. each category for each categorical variable becomes a binary variable coded 0/1.\n# Remove the intercept, because model.matrix() by default includes an intercept column.\nmatrix_train <- model.matrix(income ~ . -1 -education_num, data = df_train)\ny_train <- df_train$income\nmatrix_test <- model.matrix(income ~ . -1 -education_num, data = df_test)\ny_test <- df_test$income\n\n\nI do cross-validation to determine the best Lambda value for training the model. The trained model is used to make predictions in the test data. I decide to use the lambda that gives the minimum binomial deviance.\n\n\nCode\n# Fit Ridge regression (alpha = 0 means Ridge in glmnet)\nfit_ridge <- glmnet(matrix_train, y = y_train, family = \"binomial\", alpha = 0)\n\n# Print a summary of the model\nprint(fit_ridge)\n\n# Use cross-validation to find the best lambda (regularization parameter)\ncv_ridge <- cv.glmnet(matrix_train, y_train, family = \"binomial\", alpha = 0)\n\n# Plot the cross-validation results\nplot(cv_ridge)\n\n\n\n\n\nResults of cross-validation to find the best lambda for the ridge regression model.\n\n\n\n\n\n\nCode\n# Find the best lambda that minimizes the cross-validated error\nbest_lambda <- cv_ridge$lambda.min\n\n# Refit the model using the best lambda\nfit_ridge_best <- glmnet(matrix_train, y_train, family = \"binomial\", alpha = 0, lambda = best_lambda)\n\n# Make predictions on the test set\nprobs_ridge_best <- predict(fit_ridge_best, matrix_test, type = \"response\")\n\n# Convert probabilities to class predictions.\npred_ridge <- ifelse(probs_ridge_best > 0.5, 1, 0)\n\n## Create a confusion matrix to evaluate peformance\nconfmatrix_ridge <- confusionMatrix(table(pred_ridge, y_test), positive = \"1\")\n\n# Extract performance metrics\nspecificity_ridge <- confmatrix_ridge$byClass[\"Specificity\"]\nsensitivity_ridge <- confmatrix_ridge$byClass[\"Sensitivity\"]\naccuracy_ridge <- as.numeric(confmatrix_ridge$overall[\"Accuracy\"])\nprecision_ridge <- round(as.numeric(confmatrix_ridge$byClass[\"Pos Pred Value\"]), 3)\n\n# Use the prediction function from the ROCR package to create a prediction object. This will be used later to create an ROC graph\n# Create a performance object for the ROC curve\n# tpr = true positive rate, fpr = false positive rate\n# Calculate the AUC (Area Under the Curve)\npred_object_ridge <- prediction(probs_ridge_best, df_test$income)\nperf_ridge <- performance(pred_object_ridge, \"tpr\", \"fpr\")\nauc_ridge <- performance(pred_object_ridge, measure = \"auc\")@y.values[[1]]\n\n\nI determine the importance of the variables in this model.\n\n\nCode\n# Extract coefficients for plotting variable importance\n# Extract the coefficients for the best lambda\ncoefficients_ridge <- coef(fit_ridge_best, s = best_lambda)\n\n# Convert the coefficient matrix to a data frame for easier plotting\ncoefficients_ridge_df <- as.data.frame(as.matrix(coefficients_ridge))\ncoefficients_ridge_df$Variable <- rownames(coefficients_ridge_df)\ncolnames(coefficients_ridge_df) <- c(\"Coefficient\", \"Variable\")\n\n# Remove the intercept from the plot (optional)\ncoefficients_ridge_df <- coefficients_ridge_df[coefficients_ridge_df$Variable != \"(Intercept)\", ]\n\n# Sort the data frame by the absolute value of the coefficients and select the top 20\ncoefficients_ridge_top_20 <- coefficients_ridge_df %>%\n  arrange(desc(abs(Coefficient))) %>%\n  head(20)\n\n# Plot the top 20 coefficients for Ridge regression\nimportance_plot_ridge <- ggplot(coefficients_ridge_top_20, aes(x = reorder(Variable, abs(Coefficient)), y = Coefficient)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(Coefficient > 0, \"Positive\", \"Negative\"))) +\n  coord_flip() +\n  labs(title = \"Ridge Regression Variable Importance\",\n       x = \"Variables\", y = \"Coefficient\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Negative\" = \"darkorange\", \"Positive\" = \"steelblue\"), guide = \"none\") +\n  theme(axis.text.x = element_text(size = 12),  \n        axis.text.y = element_text(size = 12))\n\n\n\n\nXGBoost\nXGBoost is a tree-based algorithm and therefore should not be particularly sensitive to collinear features.\nXGBoost requires numerical inputs so I use the model matrices created for Ridge Regression to create a DMatrix, which is XGBoost’s optimized data structure.\n\n\nCode\n# Create a training and test label for the outcome variable\ny_train_label <- as.numeric(df_train$income)-1\ny_test_label <- as.numeric(df_test$income)-1\n\n\n# Convert the data to DMatrix, which is XGBoost's optimized data structure\ntrain_xgb <- xgb.DMatrix(data = matrix_train, label = y_train_label)\ntest_xgb <- xgb.DMatrix(data = matrix_test, label = y_test_label)\n\n\nFor this XGBoost model I choose parameters that should provide a good balance between model complexity and regularization. I am not going to use time on tuning this model.\n\n\nCode\n# Set a seed for reproducibility\nset.seed(123)\n\n# Set the XGBoost parameters\n  params <- list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eta = 0.1,  # learning rate\n    max_depth = 6,\n    gamma = 1,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    eval_metric = \"error\"\n  )\n\n# Train the model\n# Prevent the output from being printed in the rendered html document by using invisible()\ninvisible({\n  fit_xgb1 <- xgb.train(\n    params = params, \n    data = train_xgb, \n    nrounds = 100, \n    watchlist = list(train = train_xgb, eval = test_xgb), \n    early_stopping_rounds = 10,\n    print_every_n = 10\n  )\n})\n\n\nMake predictions using the XGBoost model and determine model performance.\n\n\nCode\n## Make predictions\nprobs_xgb1 <- predict(fit_xgb1, newdata = test_xgb)\npred_xgb1 <- ifelse(probs_xgb1 > 0.5, 1, 0)\n\n# Confusion matrix to obtain model performance\n# Recap: sensitivity is the recall for the positive class\n# Recap: specificity is the recall for the negative class\nconfmatrix_xgb1 <- confusionMatrix(factor(pred_xgb1), factor(y_test_label), positive = \"1\")\nsensitivity_xgb1 <- confmatrix_xgb1$byClass[\"Sensitivity\"] \nspecificity_xgb1 <- confmatrix_xgb1$byClass[\"Specificity\"] \nprecision_xgb1 <- round(as.numeric(confmatrix_xgb1$byClass[\"Pos Pred Value\"]), 3)\n\n# Use the prediction function from the ROCR package to create a prediction object. This will be used later to create an ROC graph\n# Create a performance object for the ROC curve\n# tpr = true positive rate, fpr = false positive rate\n# Calculate the AUC (Area Under the Curve)\npred_object_xgb <- prediction(probs_xgb1, y_test_label)\nperf_xgb <- performance(pred_object_xgb, \"tpr\", \"fpr\")\nauc_xgb <- performance(pred_object_xgb, measure = \"auc\")@y.values[[1]]\n\n\nAs for the other models, I determine the importance of the different variables.\n\n\nCode\n# Feature importance\nimportance_xgb <- xgb.importance(feature_names = colnames(train_xgb), model = fit_xgb1)\n\n# Prepare a plot of the top 20 most important features\n# This function (with a ggplot-backend) performs 1-D clustering of the importance values.\n# Clusters with similar importance values get the same bar colours.\nimportance_plot_xgb <- xgb.ggplot.importance(importance_matrix = importance_xgb,\n                                         top_n = 20, n_clusters = 5) + \n                    theme_minimal() + \n                    theme(\n                      axis.text.x = element_text(size = 12),  \n                      axis.text.y = element_text(size = 12),  \n                      plot.title = element_text(size = 14)\n                                                #, face = \"bold\") \n                      ) +\n                    ggtitle(\"XGBoost Variable Importance\")\n\n\n\n\nSupport Vector Classifier\nThe support vector classifier (SVC) is a method that constructs a set of hyperplanes that separates the training data into two classes.\nI perform regularization by adjusting the cost tuning parameter (C), which allows changing the number and severity of the violations to the margin. A smaller C value increases the margin size by allowing more violations, while a larger C value reduces the margin and penalizes misclassifications more severely.\n\n\n\n\nThe best value for the cost tuning parameter (C) is determined through cross validation.\n\n\nCode\n# Train the SVC model at different values of the C tuning parameter\n# I use the built-in tune() function in the e1071 library\n# Train an SVM model with a linear kernel - corresponds to support vector classifier (SVC) \n# I explicitly remove education_num from the data frame, since with first run, just defining the formula, is not sufficient for excluding it from the model.\n\n# Remove education_num from the dataset before fitting the model\ndf_train_svc <- df_train[, !colnames(df_train) %in% c(\"education_num\")]\ndf_test_svc <- df_test[, !colnames(df_test) %in% c(\"education_num\")]\n\nsvcf1 <- income ~. \nset.seed(123)\n\n# Script for finding the optimal cost parameter through cross validation\ntune_svc <- tune(svm, svcf1, data = df_train, kernel = \"linear\",\n                    ranges = list(cost = c(0.001, 0.1, 1, 10)))\n  \n# This computation takes quite some time, so I am saving the results.\n# saveRDS(tune_svc, file = \"posts/classification_adult_data/tune_svc.rds\")\n\n\nSince the cross-validation for different values takes quite some time, I save the results for easy loading. I use the cost value that gives the smallest error for subsequent predictions.\n\n\nCode\n#~/Dropbox/Projects/Portfolio/posts/classification_adult_data\n#tune_svc <- readRDS(\"/Users/Dropbox/Projects/Portfolio/posts/classification_adult_data/tune_svc.rds\")\ntune_svc <- readRDS(\"tune_svc_new.rds\")\nsummary(tune_svc)\n\n# Extract the best model\nbestmod_svc <- tune_svc$best.model\n\n# Predict the class and probabilities using the best model\npred_svc <- predict(bestmod_svc, df_test, decision.values = TRUE)\n\n## Create confusion matrix to evaluate the model's performance\n# Extract accuracy, precision, sensitivity and specificity\nconfmatrix_svc <- confusionMatrix(pred_svc, df_test$income, positive = \"1\")\naccuracy_svc <- as.numeric(confmatrix_svc$overall[\"Accuracy\"])\nprecision_svc <- round(as.numeric(confmatrix_svc$byClass[\"Pos Pred Value\"]), 3)\nspecificity_svc <- confmatrix_svc$byClass[\"Specificity\"]\nsensitivity_svc <- confmatrix_svc$byClass[\"Sensitivity\"]\n\n## ROC curve\n# Extract the decision values (distance from the decision boundary)\ndecision_values_svc <- attributes(pred_svc)$decision.values\n\n# Use ROCR package to plot the ROC curve\n# Create a prediction object using decision values and true labels\n# Use the negative of the fitted decision values so that negative values correspond to class 1 and the positive values to class 2 (ISLR p. 395).\n# Create a performance object for the ROC curve\n# Calculate the AUC (Area Under the Curve)\npred_object_svc <- prediction(-decision_values_svc, y_test_label)\nperf_svc <- performance(pred_object_svc, \"tpr\", \"fpr\")\nauc_svc <- performance(pred_object_svc, measure = \"auc\")@y.values[[1]]\n\n\nDetermine the importance of the variables in this model.\n\n\nCode\n# Determine the importance of the different variables in the model\n# Extract the coefficients (weights) and support vectors, multiplying them gives the final coefficient (weight) for each feature in the decision boundary.\nsvc_coefficients <- t(bestmod_svc$coefs) %*% bestmod_svc$SV\n\n# Extract the coefficients and the names\nsvc_coeff_values <- as.vector(svc_coefficients)\nvariable_names <- dimnames(svc_coefficients)[[2]]\n\n# Combine the names and coefficients into a named vector\n# Remember that in this case:\n# Positive coefficients (in the output from my SVM model) will push the decision toward the negative class.\n# Negative coefficients will push the decision toward the positive class.\nsvc_coefficients_named <- setNames(-svc_coeff_values, variable_names)\n\n# Sort the coefficients by their absolute values, keeping the names intact\n# Select the top 20 largest estimates (based on absolute values)\nsvc_coefficients_ordered <- svc_coefficients_named[order(abs(svc_coefficients_named), decreasing = TRUE)]\nimportance_svc_top20 <- svc_coefficients_ordered[1:20]\n  \n# Convert the named vector of top 20 into a dataframe for plotting\nimportance_svc_top20 <- data.frame(\n  Variable = names(importance_svc_top20),\n  Coefficient = importance_svc_top20\n)\n  \n# Create a bar plot with the actual values (not the absolute ones)\nimportance_plot_svc <- ggplot(importance_svc_top20, aes(x = reorder(Variable, abs(Coefficient)), y = Coefficient)) +\n    geom_bar(stat = \"identity\", aes(fill = ifelse(Coefficient > 0, \"Positive\", \"Negative\"))) +\n    coord_flip() +\n    labs(title = \"SVC Variable Importance\", \n         x = \"Variables\", \n         y = \"Coefficient Value\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"Negative\" = \"darkorange\", \"Positive\" = \"steelblue\"), guide = \"none\") +\n  theme(axis.text.x = element_text(size = 12),  \n        axis.text.y = element_text(size = 12))"
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#results",
    "href": "posts/classification_adult_data/index_adult_data.html#results",
    "title": "Statistical Learning: classification problem",
    "section": "Results",
    "text": "Results\nThe main goal of this post is to identify which statistical method performs best in predicting annual income in the adult census data. Nevertheless, I still think it is interesting to look at the variable importance across the different methods.\nThe figures below show the 20 most important variables in the different methods used.\n\n\nCode\nimportance_plot_glm\nimportance_plot_ridge\nimportance_plot_xgb\nimportance_plot_svc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below shows the ROC curves and AUC for the different methods.\n\n\nCode\nplot(perf_svc, col = \"green\", main = \"ROC Curves\", lwd = 0.8)\nplot(perf_xgb, col = \"red\", lwd = 0.8, add = TRUE)\nplot(perf_glmf2, col = \"blue\", lwd = 0.8, add = TRUE)\nplot(perf_ridge, col = \"purple\", lwd = 0.8, add = TRUE)\n\n# Add a legend to the plot with AUC values\nlegend(\"bottomright\",\n       legend = c(paste(\"SVM AUC:\", round(auc_svc, 3)),\n                  paste(\"XGBoost AUC:\", round(auc_xgb, 3)),\n                  paste(\"Logistic Regression AUC:\", round(auc_glmf2, 3)),\n                  paste(\"Ridge Regression AUC:\", round(auc_ridge, 3))),\n       col = c(\"green\", \"red\", \"blue\", \"purple\"),\n       lwd = 2)\n\n\n\n\n\nThe following table shows the performance of the different methods used as measured by the metrics sensitivity, specificity and precision.\n\n\nCode\n# Collect all the results in a data frame\nresults <- data.frame(\n  Method = c(\"Logistic Regression\", \"Ridge Regression\", \"XGBoost\", \"Support Vector Classifier\"),\n  Sensitivity = c(sensitivity_glmf2, sensitivity_ridge, sensitivity_xgb1, sensitivity_svc),\n  Specificity = c(specificity_glmf2, specificity_ridge, specificity_xgb1, specificity_svc),\n  Precision = c(precision_glmf2, precision_ridge, precision_xgb1, precision_svc)\n)\n\n# Order the results by Precision in descending order\nresults <- results[order(-results$Precision), ]\n\n# Round the values for a clean display\nresults[, 2:4] <- round(results[, 2:4], 3)"
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#discussion",
    "href": "posts/classification_adult_data/index_adult_data.html#discussion",
    "title": "Statistical Learning: classification problem",
    "section": "Discussion",
    "text": "Discussion\nWhereas logistic regression can give important information for making inferences about the importance of the different variables and their effects, XGBoost and the SVC are not designed for making direct inferences. XGBoost and the SVC are primarily predictive models that focus on maximizing predictive accuracy."
  },
  {
    "objectID": "posts/classification_adult_data/index_adult_data.html#skills",
    "href": "posts/classification_adult_data/index_adult_data.html#skills",
    "title": "Statistical Learning: classification problem",
    "section": "Skills",
    "text": "Skills\nStatistical learning, machine learning, R-programming, Tableau and data visualization.\nReferences:\nBecker, Barry and Kohavi, Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20.\nJames, G., Witten, D., Hastie, T. og Tibshirani, R. 2021. An Introduction to Statistical Learning (ISLR): with Applications in R. Available at: https://link.springer.com/content/pdf/10. 1007/978-1-0716-1418-1.pdf."
  },
  {
    "objectID": "posts/welcome/index_welcome.html",
    "href": "posts/welcome/index_welcome.html",
    "title": "Welcome To My Portfolio",
    "section": "",
    "text": "Welcome to my portfolio created with Quarto 😀!\nI created this portfolio to practice and improve my data analysis or data science skills, while simultaneously building a showcase of my work.\nI’d like to give credit to Deepsha Menghani for the instructions I followed to create this portfolio. You can find her guide here.\nThis picture has nothing to do with data analysis - I just think it is a very special flower. It is called “Koningskandelaar” (among other names), it flowers in March in the Western Cape province of South Africa. It’s scientific name is Brunsvigia orientalis (L.) Aiton ex Eckl."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Portfolio",
    "section": "",
    "text": "Statistical Learning: classification problem\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nIzel Fourie Sørensen\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Portfolio\n\n\n\n\n\n\n\nintro\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\nIzel Fourie Sørensen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Izel Fourie Sørensen",
    "section": "",
    "text": "I am passionate about working with data to uncover meaningful insights that can inform decision-making. With a degree in Pharmacy, PhD in Pharmaceutical Chemistry and professional bachelor’s degree in Data Analysis, I bring a solid foundation in both science and data analysis.\nBefore formalizing my data analysis skills, I gained hands-on experience in using R for data cleaning and analysis through collaborations with scientists at Aarhus University. During these collaborations, I applied statistical prediction models to both human and fruit fly genetic data, predicting phenotypes based on genetic information. (Relevant publications are listed in my resume.)\nThis work, in addition to my degree in data analysis, has equipped me with strong skills in utilizing R for data modeling, prediction and inference.\nThrough my experience of conducting PhD and postdoctoral research, I developed critical thinking and a methodical approach to problem-solving. Furthermore, my diverse roles as a pharmacist in a retail pharmacy, a quality assurance specialist, and an associate scientist have allowed me to refine my communication skills, enabling me to effectively convey technical information to both specialists and non-specialists.\nMy unique combination of expertise in pharmaceutical science and data analysis allows me to bridge the gap between scientific research and practical data-driven decision-making."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Izel Fourie Sørensen",
    "section": "",
    "text": "I am passionate about working with data to uncover meaningful insights that can inform decision-making. With a degree in Pharmacy, PhD in Pharmaceutical Chemistry and professional bachelor’s degree in Data Analysis, I bring a solid foundation in both science and data analysis.\nBefore formalizing my data analysis skills, I gained hands-on experience in using R for data cleaning and analysis through collaborations with scientists at Aarhus University. During these collaborations, I applied statistical prediction models to both human and fruit fly genetic data, predicting phenotypes based on genetic information. (Relevant publications are listed in my resume.)\nThis work, in addition to my degree in data analysis, has equipped me with strong skills in utilizing R for data modeling, prediction, and interpretation.\nThrough my experience of conducting PhD and postdoctoral research, I developed critical thinking and a methodical approach to problem-solving. Furthermore, my diverse roles as a pharmacist in a retail pharmacy, a quality assurance specialist, and an associate scientist have allowed me to refine my communication skills, enabling me to effectively convey technical information to both specialists and non-specialists.\nMy unique combination of expertise in pharmaceutical science and data analysis allows me to bridge the gap between scientific research and practical data-driven decision-making."
  }
]